debug: 0
seed: 808

# PARAMETER EFFICIENT FINE TUNING
USE_PEFT: False

# NUMBER OF LAYERS TO FREEZE 
# DEBERTA LARGE HAS TOTAL OF 24 LAYERS
FREEZE_LAYERS:  18
# BOOLEAN TO FREEZE EMBEDDINGS
FREEZE_EMBEDDINGS: True

# LENGTH OF CONTEXT PLUS QUESTION ANSWER
MAX_INPUT: 1750
# HUGGING FACE MODEL
MODEL: 'microsoft/deberta-v3-large'

# MODEL PARAMS
warmup_ratio: 0.1
learning_rate: 2.E-5
epochs: 10
gradient_accumulation_steps: 8
logging_steps: 25
eval_steps: 25
lr_scheduler_type: "cosine"
weight_decay: 0.01
save_total_limit: 2
save_strategy: "epoch"

output_dir: "./model_checkpoints"
name: "deberta_reward"

data_path: ""
valid_path: ""
model_checkpoint_path: ""